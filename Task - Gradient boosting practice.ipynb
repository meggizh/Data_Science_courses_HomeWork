{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting practice\n",
    "\n",
    "\n",
    "This task will use the open source `boston` dataset from `sklearn.datasets`. The last 25% of objects will be left for quality control, dividing `X` and `y` into `X_train`, `y_train` and `X_test`, `y_test`.\n",
    "\n",
    "The goal of the assignment will be to implement a simple version of gradient boosting over regression trees for the case of a quadratic loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  target  \n",
       "0     15.3  396.90   4.98    24.0  \n",
       "1     17.8  396.90   9.14    21.6  \n",
       "2     17.8  392.83   4.03    34.7  \n",
       "3     18.7  394.63   2.94    33.4  \n",
       "4     18.7  396.90   5.33    36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "%pylab inline\n",
    "#Populating the interactive namespace from numpy and matplotlib\n",
    "boston = datasets.load_boston()\n",
    "print(boston.DESCR)\n",
    "print(boston.keys())\n",
    "#print(\"feature names: {}\".format(boston.feature_names))\n",
    "#print(\"target names: {names}\".format(names = boston.target))\n",
    "from pandas import DataFrame\n",
    "boston_frame = DataFrame(boston.data)\n",
    "boston_frame.columns = boston.feature_names\n",
    "boston_frame['target'] = boston.target\n",
    "boston_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (506, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('df shape:', boston_frame.shape)\n",
    "X = boston_frame.drop('target', 1)\n",
    "y = boston_frame['target']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after split:\n",
      "X_train: (379, 13), y_train: 379,\n",
      "X_test: (127, 13), y_test: 127.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "379    10.2\n",
       "380    10.4\n",
       "381    10.9\n",
       "382    11.3\n",
       "383    12.3\n",
       "       ... \n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: target, Length: 127, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "(X_train, X_test, \n",
    " y_train, y_test) = train_test_split(X, y, \n",
    "                                     test_size=0.25, \n",
    "                                     shuffle=False)\n",
    "\n",
    "print (\"Size after split:\\nX_train: {}, y_train: {},\\nX_test: {}, y_test: {}.\".format(X_train.shape, \n",
    "                                                                                      len(y_train), X_test.shape, len(y_test)))\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using another approach for data split:\n",
    "boston = datasets.load_boston()\n",
    "X = boston.data \n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, 506, y,506, Xtrain, 380, Xtest, 126, ytrain, 380, ytest, 126\n"
     ]
    }
   ],
   "source": [
    "splitting=len(X)-round(len(X)*0.25)\n",
    "\n",
    "X_train=X[0:splitting,:]\n",
    "X_test=X[splitting:,:]\n",
    "y_train=y[0:splitting]\n",
    "y_test=y[splitting:]\n",
    "print(\"X, {}, y,{}, Xtrain, {}, Xtest, {}, ytrain, {}, ytest, {}\".format(len(X),len(y), \n",
    "                                                                         len(X_train), len(X_test), \n",
    "                                                                         len(y_train), len(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.4, 10.9, 11.3, 12.3,  8.8,  7.2, 10.5,  7.4, 10.2, 11.5, 15.1,\n",
       "       23.2,  9.7, 13.8, 12.7, 13.1, 12.5,  8.5,  5. ,  6.3,  5.6,  7.2,\n",
       "       12.1,  8.3,  8.5,  5. , 11.9, 27.9, 17.2, 27.5, 15. , 17.2, 17.9,\n",
       "       16.3,  7. ,  7.2,  7.5, 10.4,  8.8,  8.4, 16.7, 14.2, 20.8, 13.4,\n",
       "       11.7,  8.3, 10.2, 10.9, 11. ,  9.5, 14.5, 14.1, 16.1, 14.3, 11.7,\n",
       "       13.4,  9.6,  8.7,  8.4, 12.8, 10.5, 17.1, 18.4, 15.4, 10.8, 11.8,\n",
       "       14.9, 12.6, 14.1, 13. , 13.4, 15.2, 16.1, 17.8, 14.9, 14.1, 12.7,\n",
       "       13.5, 14.9, 20. , 16.4, 17.7, 19.5, 20.2, 21.4, 19.9, 19. , 19.1,\n",
       "       19.1, 20.1, 19.9, 19.6, 23.2, 29.8, 13.8, 13.3, 16.7, 12. , 14.6,\n",
       "       21.4, 23. , 23.7, 25. , 21.8, 20.6, 21.2, 19.1, 20.6, 15.2,  7. ,\n",
       "        8.1, 13.6, 20.1, 21.8, 24.5, 23.1, 19.7, 18.3, 21.2, 17.5, 16.8,\n",
       "       22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "**Boosting** is a method of building compositions of basic algorithms by sequentially adding a new algorithm to the current composition with a certain coefficient.\n",
    "\n",
    "Gradient boosting trains each new algorithm so that it approximates the error anti-gradient over composition responses on the training set. Similar to function minimization using gradient descent, in gradient boosting we tweak the composition by changing the algorithm in the direction of the error anti-gradient.\n",
    "\n",
    "\n",
    "The function has the form L(a(x), y) = [a(x) - y]^2.\n",
    "\n",
    "You need to differentiate with respect to a(x). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Get an array for `DecisionTreeRegressor` objects (we will use them as basic algorithms) and for real numbers (these will be coefficients in front of the basic algorithms).\n",
    "\n",
    "In the loop from, train 50 decision trees sequentially with the parameters `max_depth=5` and `random_state=42` (other parameters are default). Boosting often uses hundreds or thousands of trees, but we will limit ourselves to 50 so that the algorithm runs faster and is easier to debug (because the goal of the task is to figure out how the method works). Each tree must be trained on the same set of objects, but the answers that the tree learns to predict will change in accordance with the rule obtained in task 1.\n",
    "\n",
    "Try to always start with a factor of 0.9. It is usually justified to choose a coefficient much smaller - about 0.05 or 0.1, but since in our training example, there will be only 50 trees on a standard dataset, let's take a larger step for a start.\n",
    "\n",
    "In the process of implementing the training, you will need a function that will calculate the forecast of the composition of trees built at the moment on the sample `X`:\n",
    "\n",
    "```\n",
    "def gbm_predict(X):\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "\n",
    "```\n",
    "\n",
    "The same function will help you get a prediction on the control sample and evaluate the performance of your algorithm using `mean_squared_error` в `sklearn.metrics`. \n",
    "\n",
    "Raise the result to the power of 0.5 to get `RMSE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.455221764927312\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, metrics, tree \n",
    "import numpy as np\n",
    "\n",
    "def gbm_predict(X):\n",
    "    return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]\n",
    "\n",
    "\n",
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "s = y_train\n",
    "\n",
    "for i in range(50):\n",
    "    #clf = tree.DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    alg = tree.DecisionTreeRegressor(max_depth=5, random_state=42) \n",
    "    alg.fit(X_train, s)\n",
    "    coefficients_list.append(0.9) \n",
    "    base_algorithms_list.append(alg) \n",
    "    s=y_train-gbm_predict(X_train)\n",
    "    \n",
    "mse = metrics.mean_squared_error(y_test, gbm_predict(X_test)) \n",
    "print(mse ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(answer, name):\n",
    "    \n",
    "    with open(name, \"w\") as fout:\n",
    "        fout.write(str(answer))\n",
    "        \n",
    "write_answer((mse ** 0.5), \"grad_boost_answer1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Try to reduce the weight before each algorithm with each next iteration according to the formula `0.9 / (1.0 + i)`, where `i` is the iteration number (from 0 to 49). Use the performance of the algorithm as **answer in point 3**.\n",
    "\n",
    "In reality, the following step selection strategy is often used: as soon as an algorithm is chosen, we select the coefficient in front of it by a numerical optimization method in such a way that the deviation from the correct answers is minimal. We will not suggest that you implement this for the task, but we recommend that you try to figure out such a strategy and implement it on occasion for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.241033584774468\n"
     ]
    }
   ],
   "source": [
    "base_algorithms_list = []\n",
    "coefficients_list = []\n",
    "s = y_train\n",
    "\n",
    "for i in range(50):\n",
    "    #clf = tree.DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    alg = tree.DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "    alg.fit(X_train, s)\n",
    "    coefficients_list.append(0.9 / (1.0 + i)) \n",
    "    base_algorithms_list.append(alg) \n",
    "    s=y_train-gbm_predict(X_train)\n",
    "    \n",
    "mse = metrics.mean_squared_error(y_test, gbm_predict(X_test)) \n",
    "print(mse ** 0.5)\n",
    "write_answer((mse ** 0.5), \"grad_boost_answer2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "The method implemented - gradient boosting over trees - is very popular in machine learning. It is present both in the `sklearn` library itself and in the third-party `XGBoost` library, which has its own Python interface. In practice, `XGBoost` performs noticeably better than `GradientBoostingRegressor` from `sklearn`, but you can use any implementation for this assignment.\n",
    "\n",
    "Investigate whether gradient boosting overfits as the number of iterations increases (and think about why) and also as the depth of the trees increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.ensemble.GradientBoostingRegressor\n",
    "(*, loss='squared_error', learning_rate=0.1, n_estimators=100, \n",
    "subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
    "init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, \n",
    "warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "\n",
    "loss{‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’}, default=’squared_error’\n",
    "Loss function to be optimized. ‘squared_error’ refers to the squared error for regression. ‘absolute_error’ refers to the absolute error of regression and is a robust loss function. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use alpha to specify the quantile).\n",
    "\n",
    "n_estimators int, default=100\n",
    "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "criterion{‘friedman_mse’, ‘squared_error’, ‘mse’, ‘mae’}, default=’friedman_mse’\n",
    "The function to measure the quality of a split. Supported criteria are “friedman_mse” for the mean squared error with improvement score by Friedman, “squared_error” for mean squared error, and “mae” for the mean absolute error. The default value of “friedman_mse” is generally the best as it can provide a better approximation in some cases.\n",
    "\n",
    "initestimator or ‘zero’, default=None\n",
    "An estimator object that is used to compute the initial predictions. init has to provide fit and predict. If ‘zero’, the initial raw predictions are set to zero. By default a DummyEstimator is used, predicting either the average target value (for loss=’squared_error’), or a quantile for the other losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10\n",
      "test quality =  7.285454097125206\n",
      "n_estimators=100\n",
      "test quality =  4.504648791660076\n",
      "n_estimators=500\n",
      "test quality =  4.425647100253689\n",
      "n_estimators=1000\n",
      "test quality =  4.465183540025864\n",
      "n_estimators=2000\n",
      "test quality =  4.532192436541141\n",
      "n_estimators=3000\n",
      "test quality =  4.447336161608743\n",
      "max_depth=2\n",
      "test quality =  4.555488352072528\n",
      "max_depth=5\n",
      "test quality =  4.721408610684704\n",
      "max_depth=10\n",
      "test quality =  5.868081532878581\n",
      "max_depth=20\n",
      "test quality =  5.727168923863321\n",
      "max_depth=50\n",
      "test quality =  5.901620839416651\n",
      "max_depth=100\n",
      "test quality =  5.586913572014026\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=10)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=10\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=100)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=100\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=500)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=500\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=1000)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=1000\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=2000)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=2000\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(n_estimators=3000)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"n_estimators=3000\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=2)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=2\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=5)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=5\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=10)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=10\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=20)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=20\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=50)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=50\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "alg = ensemble.GradientBoostingRegressor(max_depth=100)\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"max_depth=100\")\n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "\n",
    "answer3=\"2 3\"\n",
    "write_answer(answer3, \"grad_boost_answer3.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Compare the performance you get with gradient boosting to that of linear regression.\n",
    "\n",
    "To do this, train the `LinearRegression` from `sklearn.linear_model` (with default parameters) on the training set and evaluate for the predictions of the resulting algorithm on the `RMSE` test set. The quality received is the answer in **point 5**.\n",
    "\n",
    "In this example, the performance of a simple model should have been worse, but do not forget that this is not always the case. In the assignments for this course, you will also find an example of the reverse situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test quality =  7.819688142087445\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "alg = linear_model.LinearRegression()\n",
    "alg.fit(X_train, y_train)\n",
    "mse = metrics.mean_squared_error(y_test, alg.predict(X_test)) \n",
    "print(\"test quality = \", mse ** 0.5)\n",
    "write_answer(mse ** 0.5, \"grad_boost_answer4.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
